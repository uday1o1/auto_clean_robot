{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KTSr6KNsf1p"
   },
   "source": [
    "loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JwgVZ_i8RtG",
    "outputId": "781c9316-8a13-4197-bac4-b68cfaa7a2d6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338831998,
     "user_tz": -330,
     "elapsed": 22366,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"LpIOo51TriziCYxqzIZw\")\n",
    "project = rf.workspace(\"test-hmoom\").project(\"cleaning_bot_dataset_2.0\")\n",
    "version = project.version(3)\n",
    "dataset = version.download(\"voc\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pchPQ_91qas_"
   },
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UgCbKnEOpzEy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338843296,
     "user_tz": -330,
     "elapsed": 11254,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as et\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from glob import glob\n",
    "from tensorflow.keras.models import Model\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "import shutil\n",
    "import json\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from collections import Counter\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jw2l8x0W4I_H",
    "outputId": "87e227b2-1e3b-43df-a1cc-36b07a4f5953",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338843300,
     "user_tz": -330,
     "elapsed": 9,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "def rename_folder(folder_path, new_name):\n",
    "    # Get the directory containing the folder\n",
    "    parent_dir = os.path.dirname(folder_path)\n",
    "\n",
    "    # Create the new path with the new folder name\n",
    "    new_folder_path = os.path.join(parent_dir, new_name)\n",
    "\n",
    "    # Rename the folder\n",
    "    os.rename(folder_path, new_folder_path)\n",
    "\n",
    "    print(f\"Folder renamed from {folder_path} to {new_folder_path}\")\n",
    "\n",
    "# Find all matching folders using glob function\n",
    "folder_paths = glob('/content/cleaning_bot_dataset_2.0-*')\n",
    "\n",
    "if folder_paths:\n",
    "    folder_path = folder_paths[0]  # Use the first match\n",
    "    new_folder_name = 'cleaning_bot_object_dataset'  # New folder name\n",
    "    rename_folder(folder_path, new_folder_name)\n",
    "else:\n",
    "    print(\"No matching folders found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5VFTngnsyvD"
   },
   "source": [
    "splitting dataset into separate images and xml folders"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_lNkO3mPs3OR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846079,
     "user_tz": -330,
     "elapsed": 2784,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "ea7d2d60-a440-49d0-a785-4555b15bdb52"
   },
   "source": [
    "def move_files_by_extension(source_directory, target_directory, file_extension):\n",
    "    # Ensure the target directory exists (create it if it doesn't)\n",
    "    os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "    # Walk through the source directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(source_directory):\n",
    "        for file in files:\n",
    "            # Check if the file has the specified extension\n",
    "            if file.endswith(file_extension):\n",
    "                # Create full path for source and destination\n",
    "                source_path = os.path.join(root, file)\n",
    "                target_path = os.path.join(target_directory, file)\n",
    "\n",
    "                # Move the file\n",
    "                shutil.move(source_path, target_path)\n",
    "                print(f\"Moved: {source_path} to {target_path}\")\n",
    "\n",
    "def organize_files(folder_name):\n",
    "    # Define source and target directories based on the folder name\n",
    "    source_directory = os.path.join(\"/content/cleaning_bot_object_dataset/\", folder_name)\n",
    "    xml_target_directory = os.path.join(source_directory, \"xmls\")\n",
    "    images_target_directory = os.path.join(source_directory, \"images\")\n",
    "\n",
    "    # Create target directories if they don't exist\n",
    "    os.makedirs(xml_target_directory, exist_ok=True)\n",
    "    os.makedirs(images_target_directory, exist_ok=True)\n",
    "\n",
    "    # Move XML files\n",
    "    move_files_by_extension(source_directory, xml_target_directory, \".xml\")\n",
    "\n",
    "    # Move JPG files\n",
    "    move_files_by_extension(source_directory, images_target_directory, \".jpg\")\n",
    "\n",
    "organize_files(\"test\")\n",
    "organize_files(\"train\")\n",
    "organize_files(\"valid\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXmLDjXkAyym"
   },
   "source": [
    "combine train, valid, test to process together and split later after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtGvXf63A4ey",
    "outputId": "6effe33b-2f06-4ec8-e0ec-d48da60315b1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846086,
     "user_tz": -330,
     "elapsed": 6,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "# Define the source directories\n",
    "source_dirs = ['/content/cleaning_bot_object_dataset/train', '/content/cleaning_bot_object_dataset/valid', '/content/cleaning_bot_object_dataset/test']\n",
    "\n",
    "# Define the destination directory\n",
    "destination_dir = '/content/cleaning_bot_object_dataset'\n",
    "xml_dest_dir = os.path.join(destination_dir, 'xmls')\n",
    "jpg_dest_dir = os.path.join(destination_dir, 'images')\n",
    "\n",
    "# Create the destination directories if they do not exist\n",
    "os.makedirs(xml_dest_dir, exist_ok=True)\n",
    "os.makedirs(jpg_dest_dir, exist_ok=True)\n",
    "\n",
    "# Copy XML and JPG files from source directories to destination directories\n",
    "for src_dir in source_dirs:\n",
    "    xml_src_dir = os.path.join(src_dir, 'xmls')\n",
    "    jpg_src_dir = os.path.join(src_dir, 'images')\n",
    "\n",
    "    # Copy XML files\n",
    "    for filename in os.listdir(xml_src_dir):\n",
    "        src_file = os.path.join(xml_src_dir, filename)\n",
    "        dest_file = os.path.join(xml_dest_dir, filename)\n",
    "        shutil.copy(src_file, dest_file)\n",
    "\n",
    "    # Copy JPG files\n",
    "    for filename in os.listdir(jpg_src_dir):\n",
    "        src_file = os.path.join(jpg_src_dir, filename)\n",
    "        dest_file = os.path.join(jpg_dest_dir, filename)\n",
    "        shutil.copy(src_file, dest_file)\n",
    "\n",
    "print(\"Files have been successfully combined.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gYhPdjM3Dv5Q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846091,
     "user_tz": -330,
     "elapsed": 2,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "def delete_folder(folder_path):\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        # Remove the folder and all its contents\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Folder '{folder_path}' and all its contents have been deleted.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' does not exist.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8tdRwrkD4hE",
    "outputId": "90b51760-b043-4cca-88a8-399cae87163f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846103,
     "user_tz": -330,
     "elapsed": 10,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "folder_to_delete = '/content/cleaning_bot_object_dataset/train'\n",
    "delete_folder(folder_to_delete)\n",
    "\n",
    "folder_to_delete = '/content/cleaning_bot_object_dataset/valid'\n",
    "delete_folder(folder_to_delete)\n",
    "\n",
    "folder_to_delete = '/content/cleaning_bot_object_dataset/test'\n",
    "delete_folder(folder_to_delete)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORP1oOW-Ctm6",
    "outputId": "4f6b1b8b-9a05-43d8-a915-117f7e012c1d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846114,
     "user_tz": -330,
     "elapsed": 10,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "def count_files_in_folder(folder_path):\n",
    "    # List all files in the folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    return len(files)\n",
    "\n",
    "folder_path = '/content/cleaning_bot_object_dataset/xmls'\n",
    "file_count = count_files_in_folder(folder_path)\n",
    "print(f\"Number of files in '{folder_path}': {file_count}\")\n",
    "\n",
    "folder_path = '/content/cleaning_bot_object_dataset/images'\n",
    "file_count = count_files_in_folder(folder_path)\n",
    "print(f\"Number of files in '{folder_path}': {file_count}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zrR577EFEEv"
   },
   "source": [
    "build xml parser"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tnhpM6w4yg0q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846116,
     "user_tz": -330,
     "elapsed": 1,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "def parse_annotation(xml_file):\n",
    "    tree = et.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    annotation = {\n",
    "        'filename': root.find('filename').text,\n",
    "        'size': {\n",
    "            'width': int(root.find('size/width').text),\n",
    "            'height': int(root.find('size/height').text),\n",
    "            'depth': int(root.find('size/depth').text),\n",
    "        },\n",
    "        'objects': []\n",
    "    }\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        obj_data = {\n",
    "            'name': obj.find('name').text,\n",
    "            'bndbox': {\n",
    "                'xmin': int(obj.find('bndbox/xmin').text),\n",
    "                'xmax': int(obj.find('bndbox/xmax').text),\n",
    "                'ymin': int(obj.find('bndbox/ymin').text),\n",
    "                'ymax': int(obj.find('bndbox/ymax').text),\n",
    "            },\n",
    "            'polygon': []\n",
    "        }\n",
    "\n",
    "        for i in range(1, 60):  # Assuming no more than 60 points in a polygon\n",
    "            x = obj.find(f'polygon/x{i}')\n",
    "            y = obj.find(f'polygon/y{i}')\n",
    "            if x is not None and y is not None:\n",
    "                obj_data['polygon'].append((float(x.text), float(y.text)))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        annotation['objects'].append(obj_data)\n",
    "\n",
    "    return annotation\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xrte2xf8r317",
    "outputId": "e9986e61-17c6-47be-ccf6-d1f80619a0dd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846125,
     "user_tz": -330,
     "elapsed": 6,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "# Specify the folder path\n",
    "folder_path = '/content/cleaning_bot_object_dataset/xmls'\n",
    "\n",
    "# Get a list of files in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Choose a random file\n",
    "random_file = random.choice(files)\n",
    "\n",
    "# Get the full path of the chosen file\n",
    "xml_file = os.path.join(folder_path, random_file)\n",
    "# xml_file = '/content/cleaning_bot_object_dataset/xmls/standing_8_11_23-71-_jpg.rf.ee430e8bb392e115f304698468892cad.xml'\n",
    "\n",
    "parsed_data = parse_annotation(xml_file)\n",
    "\n",
    "# Pretty-print the parsed data\n",
    "print(json.dumps(parsed_data, indent=4))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8gadr_Yy8Ij"
   },
   "source": [
    "find data distribution by parsing xmls"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-D4PnIKy_Kf",
    "outputId": "41072f28-ddaa-4242-bfce-fea907ff30f8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846134,
     "user_tz": -330,
     "elapsed": 8,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "def parse_annotations_for_classes(xml_folder):\n",
    "    \"\"\"\n",
    "    Parses annotation XML files and extracts labels.\n",
    "\n",
    "    Args:\n",
    "    xml_folder: Path to the folder containing XML files\n",
    "\n",
    "    Returns:\n",
    "    A list of labels extracted from the XML files\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "\n",
    "    for xml_file in os.listdir(xml_folder):\n",
    "        if xml_file.endswith('.xml'):\n",
    "            tree = et.parse(os.path.join(xml_folder, xml_file))\n",
    "            root = tree.getroot()\n",
    "\n",
    "            for obj in root.findall('object'):\n",
    "                label = obj.find('name').text\n",
    "                # Filter out any non-relevant labels (e.g., if label is not empty)\n",
    "                if label.strip():  # Make sure it's not an empty string or just whitespace\n",
    "                    labels.append(label)\n",
    "\n",
    "    return labels\n",
    "\n",
    "def count_labels(labels):\n",
    "    \"\"\"\n",
    "    Counts occurrences of each label.\n",
    "\n",
    "    Args:\n",
    "    labels: A list of labels\n",
    "\n",
    "    Returns:\n",
    "    A pandas DataFrame with counts of each label\n",
    "    \"\"\"\n",
    "    label_counts = Counter(labels)\n",
    "    label_df = pd.DataFrame(label_counts.items(), columns=['Label', 'Count'])\n",
    "    label_df = label_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    return label_df\n",
    "\n",
    "# Define paths\n",
    "xml_folder_path = '/content/cleaning_bot_object_dataset/xmls'\n",
    "\n",
    "# Parse annotations and count labels\n",
    "labels = parse_annotations_for_classes(xml_folder_path)\n",
    "label_distribution = count_labels(labels)\n",
    "\n",
    "# Print the output\n",
    "print(label_distribution)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiRO2fNtcCnN",
    "outputId": "d914556d-a3a7-4940-dd4b-140977b9cc5b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846136,
     "user_tz": -330,
     "elapsed": 4,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "# label_mapping = {\n",
    "#     'clothes': 0,\n",
    "#     'dust_debris': 1,\n",
    "#     'electronics': 2,\n",
    "#     'furniture': 3,\n",
    "#     'household_item': 4,\n",
    "#     'liquid_spill': 5,\n",
    "#     'person': 6,\n",
    "#     'pet': 7,\n",
    "#     'small_objects': 8,\n",
    "#     'waste': 9\n",
    "# }\n",
    "\n",
    "# define label mapping\n",
    "label_mapping = {label: idx for idx, label in enumerate(label_distribution['Label'].unique())}\n",
    "\n",
    "num_classes = len(label_mapping)\n",
    "print('total classes = ', num_classes)\n",
    "# label_mapping['null'] = total_classes + 1\n",
    "print(label_mapping)\n",
    "\n",
    "# Reverse the label_mapping dictionary to map numbers to names\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuqvJazdFa1C"
   },
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0bYGOeLFQmm"
   },
   "source": [
    "create list of all parsed xml annotation data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KC9hMCO06BAC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846292,
     "user_tz": -330,
     "elapsed": 155,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7f05dddc-f9f2-4db7-a20c-4acfca70dec1"
   },
   "source": [
    "# Get all XML file paths\n",
    "xml_files = sorted(glob('/content/cleaning_bot_object_dataset/xmls/*.xml'))\n",
    "image_folder = '/content/cleaning_bot_object_dataset/images/'  # Path to the images folder\n",
    "\n",
    "y = []  # List to store the targets\n",
    "\n",
    "# Loop over each file path and parse it\n",
    "for xml_file in xml_files:\n",
    "    tree = et.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Find the object element\n",
    "    obj = root.find('object')\n",
    "\n",
    "    if obj is not None:\n",
    "        label_obj = obj.find('name').text\n",
    "\n",
    "        # Get label from the mapping\n",
    "        label = label_mapping.get(label_obj)\n",
    "\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "        # Append the results to our target list\n",
    "        y.append([label, xmin, ymin, xmax, ymax])\n",
    "    else:\n",
    "        # Determine the corresponding image file (same name as XML file, but with .jpg extension)\n",
    "        base_name = os.path.basename(xml_file).replace('.xml', '')\n",
    "        image_file = os.path.join(image_folder, f\"{base_name}.jpg\")\n",
    "\n",
    "        # Delete both the XML and the corresponding image file\n",
    "        if os.path.exists(xml_file):\n",
    "            os.remove(xml_file)\n",
    "            print(f\"Deleted XML: {xml_file}\")\n",
    "        if os.path.exists(image_file):\n",
    "            os.remove(image_file)\n",
    "            print(f\"Deleted Image: {image_file}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(y[random.randint(0, file_count)])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBuP6IsLpRsj",
    "outputId": "27345955-e1b3-431e-bf62-d6c1bef15ec4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338846337,
     "user_tz": -330,
     "elapsed": 36,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj5tl7D_FWR7"
   },
   "source": [
    "create list of all images associated to xml files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jHIa8j2s57DN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338849969,
     "user_tz": -330,
     "elapsed": 3631,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "x = []  # List to store the images\n",
    "\n",
    "# Get all image paths\n",
    "images_path = sorted(glob('/content/cleaning_bot_object_dataset/images/*.jpg'))\n",
    "\n",
    "# Loop over image paths\n",
    "for image_path in images_path:\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    x.append(img_rgb)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "CfeiIZ9N__M7",
    "outputId": "fd12ff0f-30cf-4f68-fbf3-51128a67110e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338850489,
     "user_tz": -330,
     "elapsed": 515,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "# Ensure file_count is the length of x\n",
    "file_count = len(x)\n",
    "\n",
    "# Extract random image and the corresponding bounding box coordinates\n",
    "num = random.randint(0, file_count - 1)  # Adjusted to prevent out-of-bounds error\n",
    "img_example = x[num]\n",
    "bbox = y[num][1:]  # Bounding box coordinates from the first annotation\n",
    "label = y[num][0]  # The label of the image\n",
    "label_name = reverse_label_mapping[label]\n",
    "\n",
    "def visualize_image_with_bbox(img, bbox, label, label_name):\n",
    "    # Draw the bounding box on the image\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "    cv2.putText(img, 'Label: {}'.format(label_name), (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Convert BGR to RGB for matplotlib\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Print raw data for inspection\n",
    "    print(f\"Raw image array shape: {img.shape}\")\n",
    "    print(f\"Bounding box coordinates: {bbox}\")\n",
    "    print(f\"Label: {label} ({label_name})\")\n",
    "    print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "# Visualize the image with the bounding box\n",
    "visualize_image_with_bbox(img_example, bbox, label, label_name)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDvsVpBjFjlD"
   },
   "source": [
    "split into train, test numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLU_GsRcE6FN",
    "outputId": "f86bb1b3-8b02-4010-c9a2-8d445d58b825",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338850797,
     "user_tz": -330,
     "elapsed": 304,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "source": [
    "# Convert lists to NumPy arrays\n",
    "x = np.array(x)  # Convert list of images to numpy array\n",
    "y = np.array(y)  # Convert list of labels to numpy array\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=21)\n",
    "\n",
    "# Check the shapes of the split data\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Assuming y is structured as [label, xmin, ymin, xmax, ymax]\n",
    "labels = [entry[0] for entry in y]  # Extracting only the class labels\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(\"Label counts:\", label_counts)\n",
    "print(\"Total samples in y:\", len(y))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whR7eCEMlGGh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338850844,
     "user_tz": -330,
     "elapsed": 28,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "abcc4f85-0f82-41be-c34e-6ca587d06f29"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPn8sia4I2jn"
   },
   "source": [
    "split data into separate categorical and boundary box sectors"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "categ_train = y_train[:, 0]  # Take the first column for training categories\n",
    "categ_test = y_test[:, 0]    # Take the first column for testing categories\n",
    "bbox_train = y_train[:, 1:]   # Take the remaining columns for training bounding boxes\n",
    "bbox_test = y_test[:, 1:]     # Take the remaining columns for testing bounding boxes\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"categ_train shape: {categ_train.shape}\")\n",
    "print(f\"categ_test shape: {categ_test.shape}\")\n",
    "print(f\"bbox_train shape: {bbox_train.shape}\")\n",
    "print(f\"bbox_test shape: {bbox_test.shape}\")\n",
    "\n",
    "# Convert category labels to one-hot encoded vectors\n",
    "categ_train = tf.keras.utils.to_categorical(categ_train.astype(int), num_classes=num_classes)\n",
    "categ_test = tf.keras.utils.to_categorical(categ_test.astype(int), num_classes=num_classes)\n",
    "\n",
    "# Print shapes after one-hot encoding\n",
    "print(f\"One-hot encoded categ_train shape: {categ_train.shape}\")\n",
    "print(f\"One-hot encoded categ_test shape: {categ_test.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwD8ik90kgmF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338850851,
     "user_tz": -330,
     "elapsed": 6,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "a0572c27-21c5-4846-8d0c-556caecdea89"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUxfTRfOJXPi"
   },
   "source": [
    "Loading the Single Shot Detector model with custom layers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "input_shape = (299, 299, 3)\n",
    "num_classes = 6\n",
    "learning_rate_initial = 1e-4\n",
    "batch_size = 16  # Reduced batch size for memory efficiency\n",
    "\n",
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.layers import Concatenate, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Make sure directories exist\n",
    "import os\n",
    "os.makedirs('/content/models', exist_ok=True)\n",
    "os.makedirs('./logs/ssd', exist_ok=True)\n",
    "\n",
    "# Clear session to start fresh\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define SSD-inspired model\n",
    "# 1. Base feature extractor (VGG16)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# 2. Extract features from different layers for multi-scale detection\n",
    "# Get features from different scales\n",
    "feature_maps = []\n",
    "feature_maps.append(base_model.get_layer('block4_conv3').output)  # Earlier feature map\n",
    "feature_maps.append(base_model.get_layer('block5_conv3').output)  # Later feature map\n",
    "\n",
    "# 3. Process each feature map\n",
    "processed_features = []\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    # Apply convolutional detection head to each feature map\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(feature_map)\n",
    "    processed_features.append(GlobalAveragePooling2D()(x))\n",
    "\n",
    "# 4. Concatenate features from different scales\n",
    "concatenated_features = Concatenate()(processed_features)\n",
    "x = Dense(512, activation='relu')(concatenated_features)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# 5. Classification and bounding box outputs\n",
    "classification_output = Dense(num_classes, activation='softmax', name='classification')(x)\n",
    "bounding_box_output = Dense(4, activation='linear', name='bounding_box')(x)\n",
    "\n",
    "# 6. Create model\n",
    "ssd_model = Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=[classification_output, bounding_box_output]\n",
    ")\n",
    "\n",
    "# 7. Freeze early layers for transfer learning\n",
    "for layer in base_model.layers[:10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 8. Compile the model\n",
    "ssd_model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate_initial),\n",
    "    loss={\n",
    "        'classification': 'categorical_crossentropy',\n",
    "        'bounding_box': 'mse'\n",
    "    },\n",
    "    metrics={\n",
    "        'classification': 'accuracy',\n",
    "        'bounding_box': 'mse'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "ssd_model.summary()\n",
    "\n",
    "# Define the learning rate scheduler function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 42:\n",
    "        return float(lr)  # Keep learning rate constant for first 42 epochs\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.12))  # Exponential decay of learning rate\n",
    "\n",
    "# Define callbacks\n",
    "callback_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# Specify the path where you want to save the model\n",
    "checkpoint_path = '/content/models/ssd_best_model.keras'\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "# TensorBoard for visualization\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs/ssd')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9keCeQoQK77J",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745338859660,
     "user_tz": -330,
     "elapsed": 8808,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "7a5e3cda-fb8c-4b8d-def2-4516e48c6fbc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBgyoga1qQiN"
   },
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "r = ssd_model.fit(\n",
    "    x_train,  # Training images\n",
    "    [categ_train, bbox_train],  # Classification and bounding box labels\n",
    "    epochs=100,  # Number of epochs\n",
    "    batch_size=32,  # Batch size\n",
    "    validation_data=(x_test, [categ_test, bbox_test]),  # Validation data\n",
    "    callbacks=[callback_lr, checkpoint_callback, early_stopping, tensorboard],  # Callbacks for dynamic LR, saving, early stopping, and TensorBoard\n",
    "    verbose=1  # Show training progress\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQ-VG-0uLRqe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343521672,
     "user_tz": -330,
     "elapsed": 4630592,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "8510e9e3-79e5-422b-e54e-33f1f3ac9894"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fWk10siMbDC9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343521685,
     "user_tz": -330,
     "elapsed": 8,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "cdda6e85-f9bd-407a-d858-7e1f3aeb25ae"
   },
   "source": [
    "results = pd.DataFrame(r.history)\n",
    "results.tail()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "results.columns"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VOjcUiF1bzp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343521706,
     "user_tz": -330,
     "elapsed": 18,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "68190175-528a-4202-885a-a035276d3588"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the results DataFrame as an Excel file\n",
    "results.to_excel(\"model_results.xlsx\", index=False)"
   ],
   "metadata": {
    "id": "ppfcx4Sp1WDQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343521961,
     "user_tz": -330,
     "elapsed": 253,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the results DataFrame from an Excel file\n",
    "file_path = '/content/model_results.xlsx'  # Change this to your file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Create an 'epoch' column starting from 1\n",
    "df['epoch'] = df.index + 1  # Adding 1 to start epochs from 1\n",
    "\n",
    "# Select rows for every 5 epochs\n",
    "milestone_epochs = df.iloc[::5]  # Every 5th row (which corresponds to every 5th epoch)\n",
    "\n",
    "# Display the selected milestone epochs\n",
    "print(milestone_epochs)\n",
    "\n",
    "# If you want to save this to a new Excel file\n",
    "milestone_epochs.to_excel('milestone_epochs.xlsx', index=False)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwUugOa7-sBV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343522125,
     "user_tz": -330,
     "elapsed": 159,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "92cf2c10-853e-4202-eee8-abc20c4a22a3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7rok42Bcs18"
   },
   "source": [
    "Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./logs"
   ],
   "metadata": {
    "id": "HM_P0HRgM7kF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343522131,
     "user_tz": -330,
     "elapsed": 2,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the data from the Excel file\n",
    "file_path = '/content/model_results.xlsx'  # Update with your file path\n",
    "results = pd.read_excel(file_path)\n",
    "\n",
    "# Smoothing function (moving average)\n",
    "def smooth(series, window_size=5):\n",
    "    return series.rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "# Add smoothed columns to the results dataframe\n",
    "results['smooth_classification_accuracy'] = smooth(results['classification_accuracy'])\n",
    "results['smooth_val_classification_accuracy'] = smooth(results['val_classification_accuracy'])\n",
    "results['smooth_bounding_box_mse'] = smooth(results['bounding_box_mse'])\n",
    "results['smooth_val_bounding_box_mse'] = smooth(results['val_bounding_box_mse'])\n",
    "\n",
    "# Create a figure with 2 rows and 2 columns for subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))  # 2x2 grid of plots\n",
    "\n",
    "# Define a function to add a border to the subplots\n",
    "def add_border(ax):\n",
    "    ax.patch.set_edgecolor('black')\n",
    "    ax.patch.set_linewidth(2)\n",
    "\n",
    "# 1. Smoothed Classification Accuracy\n",
    "axes[0, 0].plot(results['smooth_classification_accuracy'], label='Smoothed Training Accuracy', color='#8B0000', linewidth=2)\n",
    "axes[0, 0].plot(results['smooth_val_classification_accuracy'], label='Smoothed Validation Accuracy', color='#006400', linewidth=2)\n",
    "axes[0, 0].set_title('Smoothed Classification Accuracy Over Epochs', fontsize=16, fontweight='bold', fontname='DejaVu Serif')\n",
    "axes[0, 0].set_xlabel('Epochs', fontsize=14, fontname='DejaVu Serif')\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=14, fontname='DejaVu Serif')\n",
    "axes[0, 0].legend(loc='best', fontsize=12)\n",
    "axes[0, 0].grid(True)\n",
    "add_border(axes[0, 0])\n",
    "axes[0, 0].text(0.5, -0.2, 'Plot 1', ha='center', va='center', fontsize=12, fontweight='bold', transform=axes[0, 0].transAxes)\n",
    "\n",
    "# 2. Smoothed Bounding Box Loss\n",
    "axes[0, 1].plot(results['smooth_bounding_box_mse'], label='Smoothed Training Loss', color='#8B0000', linewidth=2)\n",
    "axes[0, 1].plot(results['smooth_val_bounding_box_mse'], label='Smoothed Validation Loss', color='#006400', linewidth=2)\n",
    "axes[0, 1].set_title('Smoothed Bounding Box Loss Over Epochs', fontsize=16, fontweight='bold', fontname='DejaVu Serif')\n",
    "axes[0, 1].set_xlabel('Epochs', fontsize=14, fontname='DejaVu Serif')\n",
    "axes[0, 1].set_ylabel('Bounding Box MSE', fontsize=14, fontname='DejaVu Serif')\n",
    "axes[0, 1].legend(loc='best', fontsize=12)\n",
    "axes[0, 1].grid(True)\n",
    "add_border(axes[0, 1])\n",
    "axes[0, 1].text(0.5, -0.2, 'Plot 2', ha='center', va='center', fontsize=12, fontweight='bold', transform=axes[0, 1].transAxes)\n",
    "\n",
    "# 3. Combined Loss\n",
    "axes[1, 0].plot(results['loss'], label='Training Loss', color='#8B0000', linewidth=2)\n",
    "axes[1, 0].plot(results['val_loss'], label='Validation Loss', color='#006400', linewidth=2)\n",
    "axes[1, 0].set_title('Combined Loss Over Epochs', fontsize=16, fontweight='bold', fontname='DejaVu Serif')\n",
    "axes[1, 0].set_xlabel('Epochs', fontsize=14, fontname='DejaVu Serif')\n",
    "axes[1, 0].set_ylabel('Loss', fontsize=14, fontname='DejaVu Serif')\n",
    "axes[1, 0].legend(loc='best', fontsize=12)\n",
    "axes[1, 0].grid(True)\n",
    "add_border(axes[1, 0])\n",
    "axes[1, 0].text(0.5, -0.2, 'Plot 3', ha='center', va='center', fontsize=12, fontweight='bold', transform=axes[1, 0].transAxes)\n",
    "\n",
    "# 4. Training vs Validation Accuracy\n",
    "axes[1, 1].plot(results['classification_accuracy'], label='Training Accuracy', color='#8B0000', linewidth=2)\n",
    "axes[1, 1].plot(results['val_classification_accuracy'], label='Validation Accuracy', color='#006400', linewidth=2)\n",
    "axes[1, 1].set_title('Training vs Validation Accuracy Over Epochs', fontsize=16, fontweight='bold', fontname='DejaVu Serif')\n",
    "axes[1, 1].set_xlabel('Epochs', fontsize=14, fontname='DejaVu Serif')\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=14, fontname='DejaVu Serif')\n",
    "axes[1, 1].legend(loc='best', fontsize=12)\n",
    "axes[1, 1].grid(True)\n",
    "add_border(axes[1, 1])\n",
    "axes[1, 1].text(0.5, -0.2, 'Plot 4', ha='center', va='center', fontsize=12, fontweight='bold', transform=axes[1, 1].transAxes)\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the entire figure\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "MqrTSc4tq1Zk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343523099,
     "user_tz": -330,
     "elapsed": 961,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "8f902f0c-87c5-4228-abbc-008b68eda311"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Smoothing function (moving average)\n",
    "def smooth(series, window_size=5):\n",
    "    return series.rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "# Add smoothed columns to the results dataframe\n",
    "results['smooth_classification_accuracy'] = smooth(results['classification_accuracy'])\n",
    "results['smooth_val_classification_accuracy'] = smooth(results['val_classification_accuracy'])\n",
    "results['smooth_bounding_box_mse'] = smooth(results['bounding_box_mse'])\n",
    "results['smooth_val_bounding_box_mse'] = smooth(results['val_bounding_box_mse'])\n",
    "\n",
    "# Function to plot with borders, consistent style, and formatting\n",
    "def plot_metric(results, y_values, title, y_label, colors=['#8B0000', '#006400']):  # Dark Red and Dark Green\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot each value with the provided colors\n",
    "    for i, y in enumerate(y_values):\n",
    "        plt.plot(results[y], label=y.replace('_', ' ').title(), color=colors[i], linewidth=2)\n",
    "\n",
    "    # Add title, labels, and grid\n",
    "    plt.title(title, fontsize=16, fontweight='bold', fontname='DejaVu Serif')\n",
    "    plt.xlabel('Epochs', fontsize=14, fontname='DejaVu Serif')\n",
    "    plt.ylabel(y_label, fontsize=14, fontname='DejaVu Serif')\n",
    "\n",
    "    # Set tick labels font\n",
    "    plt.xticks(fontsize=12, fontname='DejaVu Serif')\n",
    "    plt.yticks(fontsize=12, fontname='DejaVu Serif')\n",
    "\n",
    "    # Add a legend with the best location\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "    # Add a grid for clarity\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Add a rectangle border around the whole plot (including labels and title)\n",
    "    plt.gca().patch.set_edgecolor('black')\n",
    "    plt.gca().patch.set_linewidth(2)\n",
    "\n",
    "    # Set white background\n",
    "    plt.gca().set_facecolor('white')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Smoothed Classification Accuracy\n",
    "plot_metric(results,\n",
    "            ['smooth_classification_accuracy', 'smooth_val_classification_accuracy'],\n",
    "            'Smoothed Classification Accuracy Over Epochs',\n",
    "            'Smoothed Accuracy')\n",
    "\n",
    "# Smoothed Bounding Box Loss\n",
    "plot_metric(results,\n",
    "            ['smooth_bounding_box_mse', 'smooth_val_bounding_box_mse'],\n",
    "            'Smoothed Bounding Box Loss Over Epochs',\n",
    "            'Smoothed Bounding Box MSE')\n",
    "\n",
    "# Combined Loss Over Epochs (no smoothing needed)\n",
    "plot_metric(results,\n",
    "            ['loss', 'val_loss'],\n",
    "            'Combined Loss Over Epochs',\n",
    "            'Loss')\n",
    "\n",
    "# Training vs Validation Accuracy (unsmoothed)\n",
    "plot_metric(results,\n",
    "            ['classification_accuracy', 'val_classification_accuracy'],\n",
    "            'Training vs Validation Accuracy Over Epochs',\n",
    "            'Accuracy')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1g9o0GFJoi1U",
    "outputId": "5b4964fb-0480-438d-a6cc-11d8c8711692",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343523567,
     "user_tz": -330,
     "elapsed": 462,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the results DataFrame from an Excel file\n",
    "file_path = '/content/model_results.xlsx'  # Change this to your file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to understand its structure\n",
    "print(df.columns)\n",
    "\n",
    "# Create an 'epoch' column starting from 1\n",
    "df['epoch'] = df.index + 1  # Adding 1 to start epochs from 1\n",
    "\n",
    "# Select rows for every 5 epochs\n",
    "milestone_epochs = df.iloc[::5]  # Every 5th row (which corresponds to every 5th epoch)\n",
    "\n",
    "# Display the selected milestone epochs\n",
    "print(milestone_epochs)\n",
    "\n",
    "# If you want to save this to a new Excel file\n",
    "milestone_epochs.to_excel('milestone_epochs.xlsx', index=False)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEb5jNdzKy1K",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343523581,
     "user_tz": -330,
     "elapsed": 12,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "3f23cdfa-4039-434e-d560-748f181a25b0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX8WuD-WjYpN"
   },
   "source": [
    "testing the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_path = '/content/models/ssd_best_model.keras'\n",
    "model = tf.keras.models.load_model(model_path)"
   ],
   "metadata": {
    "id": "acfyjzDqa_et",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343563948,
     "user_tz": -330,
     "elapsed": 581,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def load_image(path):\n",
    "    \"\"\"Load an image from a given path.\"\"\"\n",
    "    image = cv2.imread(path)\n",
    "    if image is None:\n",
    "        print(f\"Error loading image from {path}\")\n",
    "    return image\n",
    "\n",
    "def predict_random_xml_and_image(xml_folder, image_folder, model):\n",
    "    \"\"\"\n",
    "    Selects a random XML file from the xml_folder, loads the corresponding image from the image_folder,\n",
    "    and makes predictions using the provided model.\n",
    "\n",
    "    Args:\n",
    "    xml_folder: Path to the folder containing XML files.\n",
    "    image_folder: Path to the folder containing image files.\n",
    "    model: Pretrained model instance.\n",
    "    \"\"\"\n",
    "    # Get a list of XML files in the folder\n",
    "    xml_files = [f for f in os.listdir(xml_folder) if f.endswith('.xml')]\n",
    "\n",
    "    # Check if there are any XML files\n",
    "    if not xml_files:\n",
    "        print(\"No XML files found in the specified folder.\")\n",
    "        return\n",
    "\n",
    "    # Choose a random XML file\n",
    "    random_xml_file = random.choice(xml_files)\n",
    "\n",
    "    # Get the corresponding image file name\n",
    "    base_name = os.path.splitext(random_xml_file)[0]\n",
    "    image_file = os.path.join(image_folder, base_name + '.jpg')  # Change '.jpg' to the correct image extension if needed\n",
    "\n",
    "    # Check if the image file exists\n",
    "    if not os.path.exists(image_file):\n",
    "        print(f\"Image file not found: {image_file}\")\n",
    "        return\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image = load_image(image_file)\n",
    "\n",
    "    # Proceed with predictions\n",
    "    _process_image(image, model)\n",
    "\n",
    "def _process_image(image, model):\n",
    "    \"\"\"Process and predict on the loaded image.\"\"\"\n",
    "    # Resize the image to match model input shape\n",
    "    image_resized = cv2.resize(image, (299, 299))\n",
    "    image_array = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "    # Get model predictions\n",
    "    categ, bbox = model.predict(image_array)\n",
    "\n",
    "    # Print raw predictions for debugging\n",
    "    print(\"Raw category predictions:\", categ)\n",
    "    print(\"Raw bounding box predictions:\", bbox)\n",
    "\n",
    "    # Get class with the highest probability\n",
    "    categ_index = np.argmax(categ)\n",
    "    print(\"Predicted class index:\", categ_index)\n",
    "    print(\"Predicted class:\", reverse_label_mapping[categ_index])\n",
    "\n",
    "    # Flatten and convert bbox coordinates to integers\n",
    "    bbox = bbox.flatten().astype(int)\n",
    "    print(\"Predicted bounding box coordinates:\", bbox)\n",
    "\n",
    "    # Draw bounding box and prediction\n",
    "    image_annotated = cv2.rectangle(image_resized.copy(), (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n",
    "    prediction = reverse_label_mapping[categ_index]\n",
    "    final_img = cv2.putText(image_annotated, prediction, (bbox[0], bbox[1]-4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Display original and annotated images side by side\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Original Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Annotated Image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Annotated Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "iPFqZYRi_mMk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343571439,
     "user_tz": -330,
     "elapsed": 17,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "xml_folder_path = '/content/cleaning_bot_object_dataset/xmls'\n",
    "image_folder_path = '/content/cleaning_bot_object_dataset/images'\n",
    "\n",
    "predict_random_xml_and_image(xml_folder_path, image_folder_path, model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "CR5rvK9V_rH9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343576004,
     "user_tz": -330,
     "elapsed": 4561,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "4e24a730-5916-40fc-ad1a-227f539058b4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def load_image(path):\n",
    "    \"\"\"Load an image from a given path.\"\"\"\n",
    "    image = cv2.imread(path)\n",
    "    if image is None:\n",
    "        print(f\"Error loading image from {path}\")\n",
    "    return image\n",
    "\n",
    "def predict_random_xml_and_image(xml_folder, image_folder, model):\n",
    "    \"\"\"\n",
    "    Selects random XML files from the xml_folder, loads corresponding images from the image_folder,\n",
    "    and makes predictions using the provided model.\n",
    "\n",
    "    Args:\n",
    "    xml_folder: Path to the folder containing XML files.\n",
    "    image_folder: Path to the folder containing image files.\n",
    "    model: Pretrained model instance.\n",
    "    \"\"\"\n",
    "    xml_files = [f for f in os.listdir(xml_folder) if f.endswith('.xml')]\n",
    "\n",
    "    if not xml_files:\n",
    "        print(\"No XML files found in the specified folder.\")\n",
    "        return\n",
    "\n",
    "    # Choose a random set of 8 XML files\n",
    "    random_xml_files = random.sample(xml_files, min(8, len(xml_files)))\n",
    "\n",
    "    images = []\n",
    "    predictions = []\n",
    "\n",
    "    for random_xml_file in random_xml_files:\n",
    "        base_name = os.path.splitext(random_xml_file)[0]\n",
    "        image_file = os.path.join(image_folder, base_name + '.jpg')\n",
    "\n",
    "        if not os.path.exists(image_file):\n",
    "            print(f\"Image file not found: {image_file}\")\n",
    "            continue\n",
    "\n",
    "        image = load_image(image_file)\n",
    "        images.append(image)\n",
    "\n",
    "        # Get the model prediction\n",
    "        annotated_image, prediction = _process_image(image, model)\n",
    "        predictions.append((annotated_image, prediction))\n",
    "\n",
    "    # Display images and predictions\n",
    "    display_predictions(images, predictions)\n",
    "\n",
    "def _process_image(image, model):\n",
    "    \"\"\"Process and predict on the loaded image.\"\"\"\n",
    "    image_resized = cv2.resize(image, (299, 299))\n",
    "    image_array = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "    # Get model predictions\n",
    "    categ, bbox = model.predict(image_array)\n",
    "\n",
    "    categ_index = np.argmax(categ)\n",
    "\n",
    "    # Flatten and convert bbox coordinates to integers\n",
    "    bbox = bbox.flatten().astype(int)\n",
    "\n",
    "    # Draw bounding box on a copy of the resized image\n",
    "    image_annotated = cv2.rectangle(image_resized.copy(), (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n",
    "\n",
    "    # Get the predicted class label\n",
    "    prediction = reverse_label_mapping[categ_index]\n",
    "\n",
    "    return image_annotated, prediction\n",
    "\n",
    "def display_predictions(images, predictions):\n",
    "    \"\"\"Display images with predictions below each image in a grid.\"\"\"\n",
    "    # Create a 2x4 grid for 8 images\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    fig.suptitle('Model Predictions', fontsize=16)\n",
    "\n",
    "    for i, (ax, (img, prediction)) in enumerate(zip(axs.flatten(), predictions), start=1):\n",
    "        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Set title with image number and prediction\n",
    "        ax.set_title(f'Image {i}: {prediction}', fontsize=12)\n",
    "\n",
    "    plt.tight_layout(pad=1.2, h_pad=1)\n",
    "    plt.subplots_adjust(top=0.9)  # Adjust title position\n",
    "    plt.show()\n",
    "\n",
    "# Define paths\n",
    "xml_folder_path = '/content/cleaning_bot_object_dataset/xmls'\n",
    "image_folder_path = '/content/cleaning_bot_object_dataset/images'\n",
    "\n",
    "# Call the prediction function with the model\n",
    "predict_random_xml_and_image(xml_folder_path, image_folder_path, model)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "44XvP7NVwdt9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745343661711,
     "user_tz": -330,
     "elapsed": 2680,
     "user": {
      "displayName": "Uday Arora 21BCI0254",
      "userId": "07471650612258280837"
     }
    },
    "outputId": "f5cadffe-e24d-4df2-e466-23bcb7d4c61b"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1bVwBepob0_vSJzYWlssBZhupQNJlF4LP",
     "timestamp": 1745321196456
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
